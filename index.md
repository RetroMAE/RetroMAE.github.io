---
layout: default
---

# RetroMAE
Website for [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://arxiv.org/abs/2205.12035). 

## Models
To be added. 

## Repos
[RetroMAE codebase](https://github.com/staoxiao/RetroMAE)

## Publications
- RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder, EMNLP 2022, [[Link](https://arxiv.org/abs/2205.12035)]


```
To be updated
```

```
To be updated.
```
