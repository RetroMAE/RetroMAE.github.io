---
layout: default
title: default
---

# RetroMAE
Website for [RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder](https://arxiv.org/abs/2205.12035). 

## Models
To be added. 

## Repos
[RetroMAE codebase](https://github.com/staoxiao/RetroMAE)

## Publications
To be added.




